{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7033e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.rag_graph import build_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ed0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are transformer models?', 'answer': 'Something went wrong while processing your request. Please try again.', 'documents': ['“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.'], 'route': 'generate', 'intent': 'rag', 'error': 'RAG_ERROR: TEST_RAG_FAILURE'}\n"
     ]
    }
   ],
   "source": [
    "graph = build_graph()\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"question\": \"What are transformer models?\"\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed69795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are transformer models?', 'answer': 'Something went wrong while processing your request. Please try again.', 'documents': ['“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.', '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.'], 'route': 'generate', 'intent': 'rag', 'error': 'RAG_ERROR: TEST_RAG_FAILURE'}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b753e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformer models?',\n",
       " 'answer': 'Answer:\\nTransformer models are a type of neural network architecture for natural language processing that were introduced as an alternative to recurrent neural networks (RNNs) such as LSTMs. They are inspired by the encoder‑decoder architecture used in RNNs but replace recurrence with a fully attention‑based mechanism, allowing the model to process input sequences in parallel rather than sequentially.\\n\\nSources:\\n- sample.pdf (page 3)',\n",
       " 'documents': ['“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.',\n",
       "  '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.',\n",
       "  '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.',\n",
       "  '“We are in a time where simple methods like neural networks are giving us an explosion of new \\ncapabilities,” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google \\nThe shift from RNN models like LSTM to Transformers for NLP problems \\nAt the time of the Transformer model introduction, RNNs were the preferred approach for dealing \\nwith sequential data, which is characterized by a specific order in its input. \\nRNNs function similarly to a feed-forward neural network but process the input sequentially, one \\nelement at a time. \\nTransformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead \\nof using recurrence, the Transformer model is completely based on the Attention mechanism.'],\n",
       " 'route': 'generate',\n",
       " 'intent': 'rag',\n",
       " 'error': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = build_graph()\n",
    "graph.invoke({\n",
    "    \"question\": \"What are transformer models?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab879c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Transformer models are a type of neural network architecture for natural language processing that were introduced as an alternative to recurrent neural networks (RNNs) such as LSTMs. They are inspired by the encoder‑decoder architecture used in RNNs but replace recurrence with a fully attention‑based mechanism, allowing the model to process input sequences in parallel rather than one element at a time.\n",
      "\n",
      "Sources:\n",
      "- sample.pdf (page 3)\n",
      "I don't know based on the provided documents.\n"
     ]
    }
   ],
   "source": [
    "graph = build_graph()\n",
    "\n",
    "print(graph.invoke({\n",
    "    \"question\": \"What are transformer models?\"\n",
    "})[\"answer\"])\n",
    "\n",
    "print(graph.invoke({\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "})[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431b3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretrieve(retrieve)\n",
      "\troute_docs(route_docs)\n",
      "\tintent_router(intent_router)\n",
      "\trag(rag)\n",
      "\tpodcast(podcast)\n",
      "\timage(image)\n",
      "\tfallback(fallback)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retrieve;\n",
      "\tintent_router -.-> image;\n",
      "\tintent_router -.-> podcast;\n",
      "\tintent_router -.-> rag;\n",
      "\tretrieve --> route_docs;\n",
      "\troute_docs -.-> fallback;\n",
      "\troute_docs -. &nbsp;generate&nbsp; .-> intent_router;\n",
      "\tfallback --> __end__;\n",
      "\timage --> __end__;\n",
      "\tpodcast --> __end__;\n",
      "\trag --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61ef908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Transformer models are a type of neural network architecture for natural language processing that were introduced as an alternative to recurrent neural networks (RNNs) such as LSTMs. They are inspired by the encoder‑decoder architecture used in RNNs but replace recurrence with a fully attention‑based mechanism, allowing the model to process input sequences in parallel rather than sequentially.\n",
      "\n",
      "Sources:\n",
      "- sample.pdf (page 3)\n",
      "Host: Today we discuss transformer models.\n",
      "Guest: Transformers use attention mechanisms to understand context...\n",
      "Prompt: A clean diagram showing transformer architecture with self-attention blocks.\n"
     ]
    }
   ],
   "source": [
    "graph = build_graph()\n",
    "\n",
    "print(graph.invoke({\"question\": \"What are transformer models?\"})[\"answer\"])\n",
    "\n",
    "print(graph.invoke({\"question\": \"Create a podcast explaining transformers\"})[\"answer\"])\n",
    "\n",
    "print(graph.invoke({\"question\": \"Generate an image prompt for transformer architecture\"})[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97188ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretrieve(retrieve)\n",
      "\troute_docs(route_docs)\n",
      "\tintent_router(intent_router)\n",
      "\trag(rag)\n",
      "\tpodcast(podcast)\n",
      "\timage(image)\n",
      "\tfallback(fallback)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retrieve;\n",
      "\tintent_router -.-> image;\n",
      "\tintent_router -.-> podcast;\n",
      "\tintent_router -.-> rag;\n",
      "\tretrieve --> route_docs;\n",
      "\troute_docs -.-> fallback;\n",
      "\troute_docs -. &nbsp;generate&nbsp; .-> intent_router;\n",
      "\tfallback --> __end__;\n",
      "\timage --> __end__;\n",
      "\tpodcast --> __end__;\n",
      "\trag --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf5a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
